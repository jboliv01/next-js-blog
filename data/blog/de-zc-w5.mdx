---
title: 'Data Engineering Zoomcamp, Week 5: Apache Spark'
date: '2024-03-05'
tags: ['Guide', 'Spark', 'Distributed Computing', 'Python', 'SQL', 'Docker', 'Data Engineering']
draft: true
summary: Insights into Week 5 of the DTC DE Zoomcamp covering distributed computing with Apache Spark.
---

# Prerequisites

## Running Spark in a local Docker container

### Dockerfile
```docker
# Use Ubuntu 20.04 LTS as base image
FROM ubuntu:20.04

# Avoid prompts from apt and set timezone
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# Install necessary packages
RUN apt-get update && \
    apt-get install -y wget tar git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set environment variables for Java, Spark, and Conda
ENV JAVA_HOME=/opt/jdk
ENV SPARK_HOME=/opt/spark
ENV PATH="/opt/conda/bin:${JAVA_HOME}/bin:${SPARK_HOME}/bin:${PATH}"

# Install OpenJDK 11
RUN wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz -O /tmp/openjdk-11.tar.gz && \
    mkdir -p "$JAVA_HOME" && \
    tar --extract --file /tmp/openjdk-11.tar.gz --directory "${JAVA_HOME}" --strip-components 1 && \
    rm /tmp/openjdk-11.tar.gz

# Install Miniconda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \
    bash /tmp/miniconda.sh -b -p /opt/conda && \
    rm /tmp/miniconda.sh

# Install Python 3.8, PySpark 3.3.2, Jupyter Lab, and Pandas 2.0.1
# Note: Update PySpark to match the Spark version if necessary
RUN conda install -y python=3.8 && \
    pip install pyspark==3.5.1 jupyterlab pandas==2.0.1

# Install Spark 3.5.1
RUN wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -O /tmp/spark-3.5.1-bin-hadoop3.tgz && \
    mkdir -p "$SPARK_HOME" && \
    tar xzfv /tmp/spark-3.5.1-bin-hadoop3.tgz --strip-components=1 -C "$SPARK_HOME" && \
    rm /tmp/spark-3.5.1-bin-hadoop3.tgz

# Download and add the GCS connector jar to the Spark jars directory
RUN wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar -O $SPARK_HOME/jars/gcs-connector-latest-hadoop2.jar

# Set working directory
WORKDIR /opt/spark/work-dir

# Expose ports for Jupyter Lab and Spark UI
EXPOSE 8888 4040

# Start Jupyter Lab by default
CMD ["jupyter", "lab", "--ip='0.0.0.0'", "--port=8888", "--no-browser", "--allow-root", "--NotebookApp.token=''", "--NotebookApp.password=''"]
```

### Docker Compose

In order to run our Jupyter notebook alongside Spark UI, we need to configure these services in `docker-compose.yml`. Recall in Week 1 when we used docker-compose to configure our PostgreSQL database as well as pgadmin4 which allowed us to query our database via web interface. Same thing in Week 2 when we configured Mage to run on a container. In the `docker-compose.yml` file below, I've defined multiple ports for the SparkUI application to run on as each Spark session/application we execute will be logged separately, i.e. the Spark session I've defined below may run on port `4040` where if we were to create a second app named `test` it would log to the Spark UI on port `4041`, `4042`, etc.

```python
spark = SparkSession.builder \
    .appName("Read and Write Parquet to GCS") \
    .getOrCreate()
```

```yml
version: '3'
services:
  spark-jupyter:
    build: .
    image: spark_jupyter_lab:latest  # Name and tag the image
    ports:
      - "8888:8888"  # Jupyter Lab
      - "4040:4040"  # Spark UI
      - "4041:4041"  # Spark UI for the second application
      - "4042:4042"  # Spark UI for the third application
      - "4043:4043"  # Spark UI for the fourth application
      - "4044:4044"  # Spark UI for the fifth application
    volumes:
      - ./code:/opt/spark/work-dir  # Mount the 'code' directory to the container's work directory
```

# Spark
## RDDs
Distributed collection of objects
Spark dataframes are built upon RDDs internally. This is an additional layer of abstraction Spark provides.

For example, the following code block we read a parquet file from a gcs bucket into a spark dataframe. When we call df.take(5), it will return an array of rows, in this case 5. Essentially, a datframe in Apache Spark is an abstraction built on top of RDDs. While searching up home to explain abstraction, I found a definition that I find quite insightful 

> "Abstraction, as used in computer science, is a simplified expression of a series of tasks or attributes that allow for a more defined, accessible representation of data or systems. In computer programming, abstraction is often considered a means of “hiding” additional details, external processes and internal technicalities to succinctly and efficiently define, replicate and execute a process." - [Lcom Team](https://www.learning.com/blog/examples-of-abstraction-in-everyday-life/)

Consider the Pandas python library, this is essentially an abstraction which allows us to work with and analyze data in python with ease.

In PySpark, converting a dataframe to an RDD is straightfoward, just apply `.rdd` method to your dataframe which will then allow access to RDD operations. However, you lose the DataFrame optimizations and must then manually handle the structured nature of your data.

```python
spark = SparkSession.builder \
    .appName("Read and Write Parquet to GCS") \
    .getOrCreate()

# Specify the path to the Parquet files in GCS
bucket_name="your-bucket-name"
path = f"gs://{bucket_name}/ny_taxi_data/service=green/year=2019/month=10"

spark._jsc.hadoopConfiguration().set("google.cloud.auth.service.account.json.keyfile","path-to-your-credentials.json")

df = spark.read.parquet(path)

rdd = df.rdd

rdd.take(5)
```

# Spark Cluster

We can setup a local cluster by going into our spark directory and utilizing the following shell scripts spark provides.
Within our docker container, Spark is located in `/opt/spark/` directory. Open a terminal in the docker container and run the following commands. 

- ./sbin/start-master.sh
- ./sbin/start-worker.sh
- ./bin/spark-submit

First, we need to run `start-master.sh` which will start a local standalone spark cluster at [http://localhost:8080/](http://localhost:8080/). Assuming you are the `Dockerfile` and `docker-compose` file we implemented earlier, the Spark cluster will be named something like `Spark Master at spark://f22e994ff136:7077`, `spark://f22e994ff136:7077` being the URL we will reference below prior to executing our `06_spark_sql.py` script below. `f22e994ff136` being the hostname generated for my specific Docker container.

```shell
./sbin/start-master.sh
```
After you've ran the command to start our master server, we will need to run the following command

```shell
./sbin/start-worker.sh <master-spark-URL>

# example:
# ./sbin/start-worker.sh spark://f22e994ff136:7077`
```


Simply add this line of code into your `06_spark_sql.py` script in order to authenticate to your gcs bucket.

```python
spark = SparkSession.builder \
    .appName('test') \
    .getOrCreate()

spark._jsc.hadoopConfiguration().set("google.cloud.auth.service.account.json.keyfile","/opt/spark/work-dir/dtc-de-zoomcamp-410523-cc75fbfdb8b8.json")
```

```shell
export MASTER_URL="spark://f22e994ff136:7077"
export BUCKET_NAME="mage-zoomcamp-jonah-oliver"
export GREEN_PATH="gs://${BUCKET_NAME}/ny_taxi_data/service=green/year=2019/month=*"
export YELLOW_PATH="gs://${BUCKET_NAME}/ny_taxi_data/service=yellow/year=2019/month=*"
export OUTPUT_PATH="gs://${BUCKET_NAME}/ny_taxi_data/report/revenue/year=2019"
```

```shell
spark-submit \
    --master="${MASTER_URL}" \
        06_spark_sql.py \
            --input_green="${GREEN_PATH}" \
            --input_yellow="${YELLOW_PATH}" \
            --output="${OUTPUT_PATH}"

```
