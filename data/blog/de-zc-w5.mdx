---
title: 'Data Engineering Zoomcamp, Week 5: Apache Spark'
date: '2024-03-05'
tags: ['Guide', 'Spark', 'Distributed Computing', 'Python', 'SQL', 'Docker', 'Data Engineering']
draft: true
summary: Insights into Week 5 of the DTC DE Zoomcamp covering distributed computing with Apache Spark.
---

# Prerequisites

## Running Spark in a local Docker container

### Dockerfile
```docker
# Use Ubuntu 20.04 LTS as base image
FROM ubuntu:20.04

# Avoid prompts from apt and set timezone
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# Install necessary packages
RUN apt-get update && \
    apt-get install -y wget tar git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set environment variables for Java, Spark, and Conda
ENV JAVA_HOME=/opt/jdk
ENV SPARK_HOME=/opt/spark
ENV PATH="/opt/conda/bin:${JAVA_HOME}/bin:${SPARK_HOME}/bin:${PATH}"

# Install OpenJDK 11
RUN wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz -O /tmp/openjdk-11.tar.gz && \
    mkdir -p "$JAVA_HOME" && \
    tar --extract --file /tmp/openjdk-11.tar.gz --directory "${JAVA_HOME}" --strip-components 1 && \
    rm /tmp/openjdk-11.tar.gz

# Install Miniconda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \
    bash /tmp/miniconda.sh -b -p /opt/conda && \
    rm /tmp/miniconda.sh

# Install Python 3.8, PySpark 3.3.2, Jupyter Lab, and Pandas 2.0.1
# Note: Update PySpark to match the Spark version if necessary
RUN conda install -y python=3.8 && \
    pip install pyspark==3.5.1 jupyterlab pandas==2.0.1

# Install Spark 3.5.1
RUN wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -O /tmp/spark-3.5.1-bin-hadoop3.tgz && \
    mkdir -p "$SPARK_HOME" && \
    tar xzfv /tmp/spark-3.5.1-bin-hadoop3.tgz --strip-components=1 -C "$SPARK_HOME" && \
    rm /tmp/spark-3.5.1-bin-hadoop3.tgz

# Download and add the GCS connector jar to the Spark jars directory
RUN wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar -O $SPARK_HOME/jars/gcs-connector-latest-hadoop2.jar

# Set working directory
WORKDIR /opt/spark/work-dir

# Expose ports for Jupyter Lab and Spark UI
EXPOSE 8888 4040

# Start Jupyter Lab by default
CMD ["jupyter", "lab", "--ip='0.0.0.0'", "--port=8888", "--no-browser", "--allow-root", "--NotebookApp.token=''", "--NotebookApp.password=''"]
```

### Docker Compose

In order to run our Jupyter notebook alongside Spark UI, we need to configure these services in `docker-compose.yml`. Recall in Week 1 when we used docker-compose to configure our PostgreSQL database as well as pgadmin4 which allowed us to query our database via web interface. Same thing in Week 2 when we configured Mage to run on a container. In the `docker-compose.yml` file below, I've defined multiple ports for the SparkUI application to run on as each Spark session/application we execute will be logged separately, i.e. the Spark session I've defined below may run on port `4040` where if we were to create a second app named `test` it would log to the Spark UI on port `4041`, `4042`, etc.

```python
spark = SparkSession.builder \
    .appName("Read and Write Parquet to GCS") \
    .getOrCreate()
```

```yml
version: '3'
services:
  spark-jupyter:
    build: .
    image: spark_jupyter_lab:latest  # Name and tag the image
    ports:
      - "8888:8888"  # Jupyter Lab
      - "4040:4040"  # Spark UI
      - "4041:4041"  # Spark UI for the second application
      - "4042:4042"  # Spark UI for the third application
      - "4043:4043"  # Spark UI for the fourth application
      - "4044:4044"  # Spark UI for the fifth application
    volumes:
      - ./code:/opt/spark/work-dir  # Mount the 'code' directory to the container's work directory
```

# Spark
## RDDs
Distributed collection of objects
Spark dataframes are built upon RDDs internally. This is an additional layer of abstraction Spark provides.

For example, the following code block we read a parquet file from a gcs bucket into a spark dataframe. When we call df.take(5), it will return an array of rows, in this case 5. Essentially, a datframe in Apache Spark is an abstraction built on top of RDDs. While searching up home to explain abstraction, I found a definition that I find quite insightful 

> "Abstraction, as used in computer science, is a simplified expression of a series of tasks or attributes that allow for a more defined, accessible representation of data or systems. In computer programming, abstraction is often considered a means of “hiding” additional details, external processes and internal technicalities to succinctly and efficiently define, replicate and execute a process." - [Lcom Team](https://www.learning.com/blog/examples-of-abstraction-in-everyday-life/)

Consider the Pandas python library, this is essentially an abstraction which allows us to work with and analyze data in python with ease.

In PySpark, converting a dataframe to an RDD is straightfoward, just apply `.rdd` method to your dataframe which will then allow access to RDD operations. However, you lose the DataFrame optimizations and must then manually handle the structured nature of your data.

```python
spark = SparkSession.builder \
    .appName("Read and Write Parquet to GCS") \
    .getOrCreate()

# Specify the path to the Parquet files in GCS
bucket_name="your-bucket-name"
path = f"gs://{bucket_name}/ny_taxi_data/service=green/year=2019/month=10"

spark._jsc.hadoopConfiguration().set("google.cloud.auth.service.account.json.keyfile","path-to-your-credentials.json")

df = spark.read.parquet(path)

rdd = df.rdd

rdd.take(5)
```
