---
title: Data Engineering Zoomcamp, Week 3
date: '2024-02-07'
tags: ['Guide', 'BigQuery', 'Mage', 'Python', 'GCP','ETL']
draft: true
summary: Detailed insights from Week 2 of the Data Engineering Zoomcamp. Covering workflow orchestration with Mage, specifically using Python and PostgreSQL.
---


Data Warehouse, Data Lake, Data Mart, Data Lakehouse.. the list goes on. if your anything like me, you may be 
frequently questioning the differences and distinctions between these architectures. In fact, I've noticed
there is a common theme as I'm learning more and more about Data Engineering that there isn't generally a consensus on definitions, for better or worse. So I'll try my best 
to describe these things at a high level, before I ultimately looking up their definitions once again in the
near future:

- **Data Warehouse**: Think of this like a neatly organized library where books are arranged and cataloged.
- **Data Lake**: A huge storage room, where books are in boxes and you must dig through to find what you want.
- **Data Mart**: This is a section of our library, except it is dedicated to a particular subject. 
- **Data Lakehouse**: This is a blend of a DW and a DL, where we get the organized shelves of a DW and the vast storage of a DL in one place.

Now there is surely more to it than that, but I think it's best we try to understand things simply prior to getting into the nitty gritty, which for the purpose of this blog post, I have no desire to do. 
What I do want to discuss is my experience with Week 3 of the Data Talks Club Data Engineering Zoomcamp, in which we utilize Google's cloud-based Data Warehouse, BigQuery, to create a Machine Learning model allowing us to predict
tip amounts for theoretical taxi rides. Not only that, but we also covered query optimization with BigQuery using two concenpts none as partitioning and clustering.

# Exporting NY Taxi Data into our Google Cloud Bucket

In the previous Week 2 Module, we used Mage (a data orchestration framework) to extract, transform and load our data into PostgreSQL as well as Google BigQuery.
This week, we will further capitalize on the knowledge we've built to export *all* of the necessary data for this module into a Google Cloud Bucket.
This is a pivotal step for completing the module, as well as the homework and getting this correct will save you trouble with some of the issues I had with BigQuery SQL by doing this step incorrectly.

I'll provide two solutions, as I implemented a custom python script export the data, but I realize most people probably opted to use Mage.

The goal is to ingest Yellow Taxi Data for the years 2019 and 2020, which are used in the Week 3 Video Modules. Lastly, we will ingest
the 2022 Green Taxi Data, which is necessary to complete the Week 3 Homework assignment.

## Mage Pipeline


### Data Loader
### Transformer
### Loader

## Python Ingestion Script (Optional)
For the script below, I modified `web_to_gcs.py`, which you can find in the original DTC DE Zoomcamp repo under `03-data-warehouse\extras\web_to_gcs.py` that I happened to stumble across, 
although I'm not sure how many people actually used this solution. I found it needed some tweaking in order to not cause issues with mismatching schema down the line in BQ.

There are a few steps involved we must complete prior to running our script:

1. Install necessary dependencies
```shell
pip install pandas pyarrow google-cloud-storage
```
2. Set GOOGLE_APPLICATION_CREDENTIALS to your project/service-account key
```shell
Set GOOGLE_APPLICATION_CREDENTIALS='C:\downloads\dtc-de-zoomcamp-12345-123a456b789c.json'
```
3. Set GCP_GCS_BUCKET as your bucket or change default value of BUCKET
```shell
Set GCP_GCS_BUCKET='your-zoomcamp-bucket-name'
```

```python
import io
import os
import requests
import pandas as pd
import pyarrow.parquet as pq
from google.cloud import storage

# services = ['fhv','green','yellow']
init_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/'
# switch out the bucketname
BUCKET = os.environ.get("GCP_GCS_BUCKET", "dtc-data-lake-bucketname")
print(BUCKET)


def upload_to_gcs(bucket, object_name, local_file):
    """
    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python
    """
    # # WORKAROUND to prevent timeout for files > 6 MB on 800 kbps upload speed.
    # # (Ref: https://github.com/googleapis/python-storage/issues/74)
    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB
    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB

    client = storage.Client()
    bucket = client.bucket(bucket)
    blob = bucket.blob(object_name)
    blob.upload_from_filename(local_file)

def web_to_gcs(year, service):
    for i in range(12):
        # sets the month part of the file_name string
        month = '0'+str(i+1)
        month = month[-2:]

        # csv file_name
        file_name = f"{service}_tripdata_{year}-{month}.parquet"
        # request url for week 3 homework
        request_url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{service}_tripdata_{year}-{month}.parquet'
        print(request_url)
        #request_url = f"{init_url}{service}/{file_name}"
        r = requests.get(request_url)
        open(file_name, 'wb').write(r.content)
        print(f"Local: {file_name}")

        df = pq.read_table(file_name)
        #df.to_parquet(file_name, engine='pyarrow')
        print(f"Parquet: {file_name}")
        # upload it to gcs 
        upload_to_gcs(BUCKET, f"{service}/{file_name}", file_name)
        print(f"GCS: {service}/{file_name}")

# The following two datasets are used in the Week 3 Video Modules
web_to_gcs('2019', 'yellow')
web_to_gcs('2020', 'yellow')

# The following dataset is necessary to complete the Week 3 Homework Questions
web_to_gcs('2022', 'green')
```

Now that we've installed the necessary requirements for our script to run, set our environmental variables,
and defined our ingestion script, we can now run our script in the terminal!

Make sure you are in the directory where you saved the script prior to running this command in your shell:

```shell
python web_to_gcs.py