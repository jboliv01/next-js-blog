---
title: 'Data Engineering Zoomcamp, Week 4: Analytics Engineering with dbt'
date: '2024-02-20'
tags: ['Guide', 'dbt', 'SQL', 'CI/CD', 'Data Engineering', 'Data Visualization']
draft: true
summary: Explore key takeaways from Week 4 of the Data Engineering Zoomcamp
---

As defined on their [website](https://www.getdbt.com/product/what-is-dbt), **dbt™** is a SQL-first 
transformation workflow that lets teams quickly and collaboratively deploy analytics code following 
software engineering best practices like modularity, portability, CI/CD, and documentation. 

Allowing anyone on the data team to safely contribute to production-grade data pipelines.

enables us to apply software engineering best practices into our SQL code. Contains libraries, similar to python libraries 
that have pre written macros, similar to python functions.

# Setup

Before we can get into this module, there are a handful of datasets we need. If you haven't done this already, DTC provides a quick hack to perform this which I believe
should set you on the right path.

- 2019 and 2020 Yellow NY Taxi data
- 2019 and 2020 Green NY Taxi data
- 2019 FHV Taxi data (homework)

# Potential Issues

## Insufficient Memory

I'll start by saying I was wrong in my [Week 3 post](http://localhost:3000/blog/de-zc-w3#mage-pipeline). More specifically, I designed a Mage pipeline with the purpose of ingesting all 
of the necessary taxi data. When I initially loaded all of the data into GCS, I used [this](http://localhost:3000/blog/de-zc-w3#python-ingestion-script-optional) local python script and only ***then*** did I explore implementing the Mage pipeline. The issue is I only tested one taxi service
and one year, which seemingly worked, but on further review, I found a plethora of issues when attempting to ingest the yellow taxi data, etc. My Mage Pipeline was failing because I was loading
too much data into memory, more than my Docker container was configured to handle. This would cause the pipeline to fail and ultimately I would have to restart the docker container as it would freeze up due to memory constraints.

This lead me on a search for a solution to my memory problems. The first thing I discovered was what Mage calls `dynamic blocks`. Dynamic blocks allow you to split up blocks, such as a data loader, into multiple pieces and run them in parallel. For example, we can take our data loader and rather than concatenating 
all of our taxi data into one dataframe, we can split it into separate blocks that can each be loaded into memory in parallel or even sequentially. 

At first, I ran 4 blocks in parallel (4 data loader blocks), each block represented a parquet file containing taxi data for a specific month (i.e. Jan, Feb, March). At this point I assumed I had cracked the code, but still found my pipleine was fail due to insufficient memmory, once again. 
The solution I arrived at was to configure my Data Loader as a dynamic block and to then modify the concurrency of said blocks within the `metadata.yml` file belonging to my pipeline to a value of 1. This enabled my pipeline to run each dynamic block sequentially (1 by 1) until each file was loaded, 
rather than appending all the files to a single dataframe.

My understanding is that only 1 dynamic child block will run at a time, which will help alleviate potential memory constraints, but will be effectively much slower. Alas, I am beholden to the specifications of my personal computer, in which I've allocated
8 GBs of memory to docker as well as 4 vCPUs. Were I to configure this mage pipeline to run on a remote container, provisioned with greater memory and compute, I could likely increase the concurrency figure as necessary.


```yaml
concurrency_config:
  block_run_limit: 1
```

## Timestamp Precision

In my work with FHV taxi trip data, I encountered a notable issue during data loading. The error message was as follows:

> pyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds timestamp: 33106123800000000 

This error arises from a common situation in data handling.

Our Parquet files store timestamps in microseconds. When converting these to nanoseconds, the standard practice in Pandas, each value is multiplied by 1000 (since 1 microsecond = 1000 nanoseconds). However, this operation can push the value beyond the maximum limit of a 64-bit signed integer, used by both Pandas (datetime64[ns]) and Parquet for timestamp storage. This limit is $$2^{63}−1$$ or $$9,223,372,036,854,775,807$$. Exceeding this threshold results in an 'out of bounds' error.

For instance, our original value of 33,106,123,800,000,000 (in microseconds) becomes 33,106,123,800,000,000,000 when converted to nanoseconds, a figure represented as 3.31061238e+22. This value significantly surpasses the 64-bit limit of 9,223,372,036,854,775,807. In practical terms, the maximum year that can be represented in nanoseconds is approximately Fri Apr 11 2262 23:47:16. Our value, corresponding to February 3, 3019, at 17:30, clearly exceeds this boundary.

To illustrate these out-of-range values, refer to the following table. As a Data Engineer, encountering such erroneous timestamps necessitates a decision on handling. In our bootcamp, I chose to convert out-of-bound timestamps to Pandas' NaT (not-a-time) type, essentially treating them as null values. In real-world scenarios, alternative approaches might be considered.

| Human-Readable Date             | Microsecond Timestamp | Nanosecond Equivalent (Microsecond * 1000) | Within 64-bit Range? |
|---------------------------------|-----------------------|--------------------------------------------|----------------------|
| 1970-01-01 00:00:01             | 1,000,000             | 1,000,000,000 (1e9)                        | Yes                  |
| 2262-04-11 23:47:16             | 9,223,372,036,854     | 9,223,372,036,854,000 (9.22e15)            | Yes                  |
| 2262-04-11 23:47:16.854775      | 9,223,372,036,854,775 | 9,223,372,036,854,775,000 (9.22e18)        | Yes                  |
| Out of Bounds                   | 9,223,372,036,854,776 | 9,223,372,036,854,776,000 (9.22e18)        | No                   |
| Out of Bounds                   | 10,000,000,000,000,000| 10,000,000,000,000,000,000 (1e19)          | No                   |
| Out of Bounds                   | 33,106,123,800,000,000| 33,106,123,800,000,000,000 (3.31e19)       | No                   |


db_utils library provides us with macros we can reuse in our sql, such as the generate hash key macro. 

show an image of the resulting data model we want to build in dbt here, including fact and dim tables. 

quick description of CI/CD

quick description of DRY (dont repeat yourself) and reusable code

quick note about test suites, similar to how we implemented test assertions in Mage

dbt packages can be added in the packages.yml file, similar to how we added python dependencies for our docker container 
in the requirements.txt file in module 1.

The dbt-labs/codegen package contains a handful of useful macros that make writing a model more efficient. For example, 
we can use the `generate_model_yaml` macro to generate the YAML for a list of models we can then paste into our schema.yml file. 
This saves us time when defining the schema of our tables. https://hub.getdbt.com/dbt-labs/codegen/latest/

```shell
{% set models_to_generate = codegen.get_models(directory='marts', prefix='fct_') %}
{{ codegen.generate_model_yaml(
    model_names = models_to_generate
) }}
```

variables can be defined within the dbt_project.yml file for reuse when defining models. i.e. the payment_type_values variable
is a field that is present in both green and yellow taxi data. 

Defined as follows in `dbt_project.yml`:

```yaml
vars:
  payment_type_values: [1, 2, 3, 4, 5]
```

and later accessed in the `schema.yml` for our project under both the green and yellow taxi models using Jinja:

```yaml
 - name: payment_type
        data_type: int64
        description: ""
        tests:
          - accepted_values:
              values: " {{ var('payment_type_values') }}"
```


The following command will generate documents showing our model dependencies, DAGs, etc.

```shell
dbt docs generate
```

run the following command to build the finalized model, which will create a fact_trips table in BigQuery. 

```shell
dbt build --select +fact_trips+ --vars '{'is_test_run': 'false'}'
```


create a production environment in dbt and schedule a deploy job to run on a specified cadence

on the deploy job, enable `generate docs on run` and `run source freshness`


deploying a continious integration job in dbt. A continious integration job will ensure when any new changes 
are merged into main, a job will automatically be deployed.

![dashboard](../static/images/de-zc/w4/dashboard.png)

[link to dashboard](https://lookerstudio.google.com/embed/reporting/e6b2c2a8-4d71-440c-9c9c-e7d06651eef0/page/2b9qD)